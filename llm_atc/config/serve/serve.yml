resources:
  accelerators: A100:1
  disk_size: 1024
  disk_tier: high

file_mounts:
  /llm-atc:
    name: llm-atc # Make sure it is unique or you own this bucket name
    mode: MOUNT  # MOUNT or COPY. Defaults to MOUNT if not specified

setup: |
  conda activate chatbot
  if [ $? -ne 0 ]; then
    conda create -n chatbot python=3.9 -y
    conda activate chatbot
  fi

  # Install dependencies
  pip install vllm
  pip install git+https://github.com/lm-sys/FastChat.git
  pip install --upgrade openai
  sudo apt update
  sudo apt install -y rclone


run: |

  master_addr=`echo "$SKYPILOT_NODE_IPS" | head -n1`
  let x='SKYPILOT_NODE_RANK + 1'
  this_addr=`echo "$SKYPILOT_NODE_IPS" | sed -n "${x}p"`
  MODEL_NAME=`echo "$MODELS_LIST" | sed -n "${x}p"`

  echo "The ip address of this machine is ${this_addr}"
  echo "The head address is ${master_addr}"

  # copy files from object store onto disk
  if [[ $MODEL_NAME == llm-atc/* ]];
  then
    CHECKPOINT="/$MODEL_NAME/"
    LOCAL_CHKPT="./$MODEL_NAME/"
    mkdir -p $LOCAL_CHKPT
    rclone sync --progress --exclude "train*" $CHECKPOINT $LOCAL_CHKPT
  fi

  conda activate chatbot  

  if [ "${SKYPILOT_NODE_RANK}" == "0" ]; then
    echo 'Starting controller...'
    python -u -m fastchat.serve.controller --host 0.0.0.0 --port 21001 2>&1 | tee ~/controller.log &
    sleep 10
  else
    sleep 20
  fi
  
  echo "Starting $MODEL_NAME model worker..."
  python -u -m fastchat.serve.vllm_worker \
            --model-path $MODEL_NAME --controller-address "http://${master_addr}:21001" --host 0.0.0.0 --port 31000 --worker-address "http://${this_addr}:31000" 2>&1 | tee model_worker.log &

  echo 'Waiting for model worker to start...'
  while ! `cat model_worker.log | grep -q 'Send heart beat'`; do sleep 1; done

  if [ "${SKYPILOT_NODE_RANK}" == "0" ]; then
    echo "Starting openai_api_server server..."
    python -m fastchat.serve.openai_api_server --host 0.0.0.0 --controller-address "http://${master_addr}:21001" --port 8000
  fi
  

envs:
  MODELS_LIST: lmsys/vicuna-7b-v1.3